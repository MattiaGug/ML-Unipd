{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoYYpizuMiwJ"
   },
   "source": [
    "#Lab 2: Introduction to ScikitLearn and Classification Tasks\n",
    "\n",
    "During this Lab, we aim to achieve the following:\n",
    "\n",
    "\n",
    "*   familiarize with <a href=\"https://scikit-learn.org/stable/\"> scikit-learn </a>, an essential python library in data science;\n",
    "*   learn how to approach a classification task with scikit-learn.\n",
    "\n",
    "In this notebook, we learn to use Scikit-Learn with a practical example and then, in the second part, we will test our knowledge by doing some exercises. We will use version 1.2.2 of Scikit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9qz9Qy3aTVYw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: apt-get\n",
      "ln: /usr/bin/python: Operation not permitted\n",
      "ln: /usr/bin/python3: Operation not permitted\n",
      "Collecting numpy==1.23.5\n",
      "  Using cached numpy-1.23.5-cp39-cp39-macosx_10_9_x86_64.whl (18.1 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.4\n",
      "    Uninstalling numpy-1.22.4:\n",
      "      Successfully uninstalled numpy-1.22.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.3.0 requires daal==2021.2.3, which is not installed.\n",
      "scipy 1.7.1 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.23.5 which is incompatible.\n",
      "numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.23.5 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.23.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn==1.2.2 in /Users/mattiagugole/opt/anaconda3/lib/python3.9/site-packages (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/mattiagugole/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==1.2.2) (1.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/mattiagugole/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==1.2.2) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/mattiagugole/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==1.2.2) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/mattiagugole/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==1.2.2) (1.23.5)\n",
      "Collecting numpy>=1.17.3\n",
      "  Using cached numpy-1.22.4-cp39-cp39-macosx_10_15_x86_64.whl (17.7 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.3.0 requires daal==2021.2.3, which is not installed.\n",
      "numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.22.4 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.22.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# SETUP\n",
    "# install python at version 3.10\n",
    "!apt-get install python3.10\n",
    "\n",
    "# update symbolic links to the newly installed python version\n",
    "!ln -sf /usr/bin/python3.10 /usr/bin/python\n",
    "!ln -sf /usr/bin/python3.10 /usr/bin/python3\n",
    "\n",
    "# install numpy 1.23.5\n",
    "%pip install numpy==1.23.5\n",
    "# install scikit-learn 1.2.2\n",
    "%pip install scikit-learn==1.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YssOU-ZhNczi"
   },
   "source": [
    "# Part 1: A Classification Example With Scikit-Learn\n",
    "\n",
    "We start our lab by implementing the *perceptron* using  scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "02sO8JSRMXXZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lhTmIt8NN0ur"
   },
   "outputs": [],
   "source": [
    "X_toy, y_toy = datasets.make_blobs(n_samples=150,n_features=2,\n",
    "                           centers=2,cluster_std=1.05,\n",
    "                           random_state=123)\n",
    "y_toy[y_toy==0]=-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0q2QIo9EPe7h"
   },
   "source": [
    "We can now define our classifier: a perceptron <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html\"> [link] </a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYjwdbMaPvsX"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# we define two peceptrons: one with and one without intercept estimation (our theta_0)\n",
    "clf1 = Perceptron(fit_intercept = False)\n",
    "clf2 = Perceptron(fit_intercept = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-N6MC_b2QJ_K"
   },
   "source": [
    "Sklearn defines standard functions for models, like *fit* and *predict*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4qlKvhrQQnV"
   },
   "outputs": [],
   "source": [
    "#train phase\n",
    "clf1.fit(X_toy, y_toy)\n",
    "clf2.fit(X_toy, y_toy)\n",
    "\n",
    "#estimation (y_hat)\n",
    "y_pred_clf1 = clf1.predict(X_toy)\n",
    "y_pred_clf2 = clf2.predict(X_toy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7v6JjDVQnLH"
   },
   "source": [
    "How to evaluate our models' performance? <br>\n",
    "Scikit-learn offers a broad set of evaluation functions already implemented <a href = \"https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\">[link]</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3wse3MyQmp7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f\"CLF1 -- no intercept.\\tACC: {accuracy_score(y_toy, y_pred_clf1)}\")\n",
    "print(f\"CLF2 -- with intercept.\\tACC: {accuracy_score(y_toy, y_pred_clf2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpjsDkvVN_vD"
   },
   "source": [
    "## Model Selection\n",
    "When defining or training a model, we have the so called *hyperparameters*, i.e., different settings to configure for our training strategy.  <br>\n",
    "The question is: *how can we decide the best configuration setting for the task?* <br>\n",
    "The answer is the usage of *training* and *validation* partitions. <br>\n",
    "We can use sklearn to do that: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u53o2430OsJG"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_toy, y_toy,\n",
    "                                                  train_size = 0.8, random_state = 123)\n",
    "\n",
    "print(f\"Original size = {X_toy.shape[0]}\\tTrain size = {X_train.shape[0]}\\tVal size = {X_val.shape[0]}\")  # alternative way to use the print when there are\n",
    "                                                                                                          # variables and text to print together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SK6pa1LuRlgR"
   },
   "source": [
    "# Part 2: Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfmw2jjz4g_M"
   },
   "source": [
    "### Ex 2.1 Logistic Regression\n",
    "\n",
    "**Ex 2.1.1** Use Scikit-Learn to train a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">Logistic Regression</a> classifier with default parameters over the previously defined dataset (Lab introduction). <br>\n",
    "Compute the accuracy in both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8uueqGGSBJd"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Ex 2.1.1: complete here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwnRlnJITqat"
   },
   "source": [
    "### Ex 2.2 Logistic Regression (2)\n",
    "\n",
    "We ask you again to work on a classification task. <br>\n",
    "This time, the classification is more challenging.\n",
    "The dataset is called *sonar*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPaGZnJTTzYH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv'\n",
    "ds = pd.read_csv(url, header = None)\n",
    "\n",
    "# split into input and output elements\n",
    "data = ds.values\n",
    "X_sonar, y_sonar = data[:, :-1], data[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqKEmbmmUbRx"
   },
   "source": [
    "**EX 2.2.1** Print the shapes of our arrays $X$ and $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9jOf6l35UwUM"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Ex 2.2.1: complete here\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_8KkpEiVDH2"
   },
   "source": [
    "It's time to partition our dataset. <br>\n",
    "We ask you to create three partitions:\n",
    "\n",
    "\n",
    "*   *train set* : a set of samples used to train a model.\n",
    "*   *val set*: a set of samples used to decide the best model.\n",
    "*   *test set*: a set of samples used to see our best model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3Cl4DNbISPc"
   },
   "source": [
    "We now first split samples that we can use in our training (train and val), from samples that we cannot touch (test). <br>\n",
    "**EX 2.2.2** Create a split between train_val and test, by maintaining the 25% of samples in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "piMMVXyBVPXy"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Ex 2.2.2: complete here\n",
    "#\n",
    "# X_train_val, X_test, y_train_val, y_test ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puQHU1EiI5lW"
   },
   "source": [
    "**EX 2.2.3** From the train_val variables, split train and validation sets. Maintain the 10% of samples in the validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PK3dVG5VJJcW"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Ex 2.2.3: complete here\n",
    "#\n",
    "# X_train, X_val, y_train, y_val ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wj6e3dg3MyM6"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHh4F6EonDL4"
   },
   "source": [
    "Sklearn uses a different name for the hyperparameter $\\lambda$, can you recognise it in the documentation from its description? What is the default value for the parameter? What is its relationship with $\\lambda$?\n",
    "\n",
    "**EX 2.2.4** Train and evaluate (using accuracy) a logistic regression with the default value for the hyperparameter. Do the evaluation **only** on the training and validation partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pVIcHmWVphu"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Ex 2.2.4: complete here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRwSJs1gVsPk"
   },
   "source": [
    "This time we do not reach the 100% of accuracy in both training and validation set. <br>\n",
    "A good strategy is to apply a grid-search, i.e., find a sub-optimal hyperparameters. <br>\n",
    "We now ask you to manually implement a grid-search for *C*, an hyperparameter of the model. <br>\n",
    "See the documentation <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\"> [link] </a>. <br>\n",
    "**EX 2.2.5**  We ask you to find the best *C* among the following: $C = [0.001, 0.01, 0.1, 1., 10]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mloaRMsPWg0I"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Ex 2.2.5: complete here\n",
    "#\n",
    "C = [0.001, 0.01, 0.1, 1., 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dxp8zprVXIdL"
   },
   "source": [
    "**Ex 2.2.6** The default parameter seems to work fine. <br>\n",
    "We might want to extend the search in a smaller range: $C=[0.1, 0.5, 1, 5, 10, 15, 20] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xckj-5IXZxE"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Ex 2.2.6: complete here\n",
    "#\n",
    "C=[0.1, 0.5, 1, 5, 10, 15, 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfbEDDN2Xni3"
   },
   "source": [
    "There is no much difference, but we find sub-optimal values with $C= [1, 5, 10, 15, 20$]. <br>\n",
    "Note that while the training performance vary, the validation set is the same between these five values.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgfoeKfaYBvV"
   },
   "source": [
    "In the official documentation, we find several hyperparameters we can tune. <br>\n",
    "For example, you might want to see what happen when we do not fit the intercept. To do so, we should try both combinations (i.e., true and false) for the parameters, but by combining all the possible C we found up to now. <br>\n",
    "Since we use 7 possible values for $C$ and 2 for *fit\\_intercept*, the total number of trials are $7 * 2 = 14$. <br>\n",
    "Python-related speaking, this is translated into an inner loop, i.e., a loop inside a loop:\n",
    "\n",
    "    for c in [0.1, 0.5, 1, 5, 10, 15, 20]:\n",
    "        for fi in [True, False]:\n",
    "            #here we train and test our model\n",
    "\n",
    "If we want to find the sub-optimal among three hyper-parameters, we must add another innner loop. If we have 10 hyper-parameters, we'll have 10 inner loops! <br>\n",
    "For now, we can do it manually.\n",
    "\n",
    "**Ex 2.2.7** Find the sub-optimal values using the validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WfZkiv8dKnM"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Ex 2.2.7: complete here\n",
    "#\n",
    "\n",
    "C=[0.1, 0.5, 1, 5, 10, 15, 20]\n",
    "FI = [True, False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hfh8A6gLbWW"
   },
   "source": [
    "We don't get much improvement, We don't get much improvement, these are the same parameter values of the \"defaul\" Logistic Regression model. Don't worry about it, it just a toy-sh dataset. During the next weeks we are going to see more realistic tasks where a proper parameter selection can make the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NcUoUqSlGmV"
   },
   "source": [
    "# Ex 2.3 Grid Search Cross-Validation\n",
    "\n",
    "In the previous exercise, we implemented a grid search manually. <br>\n",
    "The more hyper-parameters, the harder to implement. <br>\n",
    "Scikit-learn eases our pain, and it offers a grid-search cross-validation function that does everything for us! <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\"> [link]</a>.<br>\n",
    "We now see together an example of grid-search implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bwxkeSVmq0z"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV #import the library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DjUPAiGmx0h"
   },
   "source": [
    "The first thing to do is to define a dictionary containing the hyper-parameters with the range of values we want to search on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyvBfn1omwSS"
   },
   "outputs": [],
   "source": [
    "param_grid_test = {\n",
    "    'C': [0.1, 0.5, 1, 5, 10, 15, 20],\n",
    "    'fit_intercept': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lw21-uMJncqa"
   },
   "source": [
    "Then, we create the grid-search object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2RghpaQtnhzw"
   },
   "outputs": [],
   "source": [
    "#target classifier\n",
    "lr = LogisticRegression()\n",
    "\n",
    "#grid-search object\n",
    "clf = GridSearchCV(estimator= lr, param_grid=param_grid_test,\n",
    "                   cv = 5, scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5Ijn5msnw4o"
   },
   "source": [
    "Finally, we can find the best model. <br>\n",
    "Note that the tool already refit the best fund model in the entire dataset (i.e., train and validation). <br>\n",
    "This is a default parameter of the grid-search CV (see the documentation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SrEmPuynwX4"
   },
   "outputs": [],
   "source": [
    "#fit the model\n",
    "clf.fit(X_toy, y_toy) #we do not use the train - validation split strategy since it is included in the CV procedure\n",
    "\n",
    "#see the best parameters and performance\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsDxM9Jopk2V"
   },
   "source": [
    "**Ex 2.3.1** Now it is your time. Implement a grid-search CV with 10 fold for the *sonar* dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mHXEHrjpxCM"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Ex 2.3.1: complete here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AXFFmIROG3v"
   },
   "source": [
    "you should see the following warning:\n",
    "\n",
    "    *ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT*\n",
    "\n",
    "**Ex 2.3.2** Use the documentation to better understand the nature of the warning.\n",
    "\n",
    "To fix it, we can set the parameter max_iter=200 in the LogisticRegression. Re-execute the previous code with this new parameter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufQ1K6BLOx_O"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Ex 2.3.2: complete here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Uo5APXyomcf"
   },
   "source": [
    "The grid-search cv returns a different Logistic Regression model, with $C=15$ and *fit_intercept=False*.  <br>\n",
    "\n",
    "**Ex 2.3.3** It's time to see this best model on the test set. Use the accuracy as evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjlRf94sM55n"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Ex 2.3.3: complete here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eu_mQdepPF_Q"
   },
   "source": [
    "# Ex 2.4: Homemade perceptron\n",
    "\n",
    "Using already made libraries is nice, they save us a lot of time!\n",
    "However, it is always interesting to understand their underlying mechanisms.\n",
    "\n",
    "In this exercise, we try to implement a perceptron using only *numpy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lLp7aPRPI96"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "#let's build a custom dataset\n",
    "X, y = datasets.make_blobs(n_samples=150,n_features=2,\n",
    "                           centers=2,cluster_std=1.05,\n",
    "                           random_state=123)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccgbgNC0P5TH"
   },
   "source": [
    "As you can see, $y$ labels are $0$ or $1$. Let's turn the zeros into $-1$ to get us into the setting we saw in class\n",
    "\n",
    "**EX 2.4.1** Modify the $y$ vector such that the labels are $\\{-1,  1\\}$ instead of $\\{0, 1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJfJKo-MQDv-"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Ex 2.4.1: complete here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1H6DvQwQK-c"
   },
   "source": [
    "We also plot what the data look like; in particular, we can see two different colours, one per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkDhE4OgQKKV"
   },
   "outputs": [],
   "source": [
    "#Plotting\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(X[:, 0][y == -1], X[:, 1][y == -1], 'r^')\n",
    "plt.plot(X[:, 0][y == 1], X[:, 1][y == 1], 'bs')\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.title('Random Classification Data with 2 classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlGf1OYQQTTe"
   },
   "source": [
    "Let's start to solve our exercise. <br>\n",
    "\n",
    "**EX 2.4.2** create two variables $m,n$ that represent the number of examples and the number of features, respectively. Get those values from $X$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqfenOW-Qqc_"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Ex 2.4.2: complete here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irc8mcuEQxRc"
   },
   "source": [
    "We now create a numpy array $\\theta$ that contains the parameters of our perceptron. <br>\n",
    "\n",
    "**EX 2.4.3** Create a numpy array *theta* filled with zero values (you should be able to determine the correct size). <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DmR9BAdiRCCr"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Ex 2.4.3: complete the solution here\n",
    "#\n",
    "# theta =\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4Yj8X4ARM-M"
   },
   "source": [
    "Add the *bias* vector to $X$ as well: <br>\n",
    "**EX 2.4.4** create a new matrix $X_{mat}$ $(m , 3)$, where the first column is set to $1$, the second and third are our $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCYfcVplRQuz"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Ex 2.4.4: complete the solution here\n",
    "#\n",
    "# X_mat =\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bj7rFJ3GFnhF"
   },
   "source": [
    "It's time to train! <br>\n",
    "**EX 2.4.5** Write a function called *perceptron_v1* that, given $X_{mat}$, $y$ and $\\theta$, updates $\\theta$ using the **perceptron** algorithm we saw in class during one of the previous lectures. <br>\n",
    "**Bonus**: we say that one algorithm has run for an epoch once it has seen (considered) all the examples in the training set once. Add a parameter  *max_epochs* to perceptron\\_v1 which stops the perceptron if it has not converged (made no error on the training set) after *max_epochs* epochs.  \n",
    "The function returns the updated $\\theta$ vector, and the number of epochs it has been executed.\n",
    "\n",
    "    #Example of how to return multiple variables\n",
    "    def example(a, b):\n",
    "        c = a + b\n",
    "        d = a * b\n",
    "        e = a / b\n",
    "        f = a - b\n",
    "        return c, d, e, f\n",
    "    \n",
    "    c, d, e, f = example(1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBWQINanFssT"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Ex 2.4.5: complete the solution here\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rgd-ykrpMdrq"
   },
   "source": [
    "Now write a function _perceptron_predict_ that computes the predictions of a trained Perceptron on a dataset, returning a vector (list) of {-1,1} values. The parameters of the function are the dataset and the theta values.\n",
    "Finally, apply _perceptron_predict_ to (X_mat, theta):\n",
    "```\n",
    "y_pred = perceptron_predict(X_mat, theta)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4JXdFP8TZBe"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Ex 2.4.5 part 2: complete the solution here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cR70ZPWzR-DB"
   },
   "source": [
    "**EX 2.4.6** Define a function called *accuracy_score* that, given the ground truth and a vector of predictions, it returns the accuracy score. <br>\n",
    "We define the accuracy as the number of correct predictions, divided by the total number of predictions. <br>\n",
    "Calculate the accuracy score of the previous perceptron execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YEKGwnBRz1_"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Ex 2.4.6: complete the solution here\n",
    "#\n",
    "\n",
    "# Expected output of the command\n",
    "# print(accuracy_score(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVX6QoCgSTER"
   },
   "source": [
    "Good job! We can finally plot the decision boundary obtained so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQWEokTbST7z"
   },
   "outputs": [],
   "source": [
    "#Plotting\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(X[:, 0][y == -1], X[:, 1][y == -1], 'r^')\n",
    "plt.plot(X[:, 0][y == 1], X[:, 1][y == 1], 'bs')\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.title('Random Classification Data with 2 classes')\n",
    "\n",
    "# ADD THE DECISION BOUNDARY\n",
    "# The Line is y=mx+c\n",
    "x1 = np.array([min(X[:,0]), max(X[:,0])])\n",
    "m = -theta[1]/theta[2]\n",
    "c = -theta[0]/theta[2]\n",
    "x2 = m * x1 + c\n",
    "plt.plot(x1, x2, 'y-')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
